---
title: 'Practical Machine Learning: Course Project'
author: "Jean-Paul Courneya"
date: "1/4/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Executive Summary
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively.

Six participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

This report describes how the data captured are used for predicting the manner in which exercise was done.

The training data were divided into two groups, a training data and validation. The validation set will validate the training model with an expected out-of-sample error rate of less than 0.5%, or 99.5% accuracy which would be acceptable for testing before it is used to perform the prediction on the 20 test cases - that must have 100% accuracy. 

The training model was developed using Random Forest method was able to achieve over 99.99% accuracy, or less than 0.03% out-of-sample error, and was able to predict the 20 test cases with 100% accuracy.

## I - Environment Prep

```{r}
library(caret)
library(knitr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
set.seed(8675309)
```

## II - Download and Pre-Process data

Download data from the URL provided. When reading in the file, change invalid values to NA.

### IIA - Download the Data

```{r, eval=FALSE}
trainURL <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
testURL <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
download.file(trainURL,destfile = 'trainset.csv')
download.file(testURL,destfile = 'testset.csv')
```

### IIB - Read the data

##### Training data
```{r}
training <- read.csv('trainset.csv', na.strings=c('NA','#DIV/0!',''))
dim(training)
```

##### Testing data
```{r}
testing <- read.csv('testset.csv', na.strings=c('NA','#DIV/0!',''))
dim(testing)
```

### IIC - Process the Training data

##### Create a partition within the training dataset 
```{r}
inTrain  <- createDataPartition(training$classe, p=0.7, list=FALSE)
TrainSet <- training[inTrain, ]
ValidationSet  <- training[-inTrain, ]
dim(TrainSet)
```

```{r}
dim(ValidationSet)
```

##### Remove variables with Nearly Zero Variance
```{r}

NZV <- nearZeroVar(TrainSet)
TrainSet <- TrainSet[, -NZV]
ValidationSet  <- ValidationSet[, -NZV]
dim(TrainSet)
```

```{r}
dim(ValidationSet)
```

##### Remove variables that are mostly NA
```{r}
AllNA    <- sapply(TrainSet, function(x) mean(is.na(x))) > 0.95
TrainSet <- TrainSet[, AllNA==FALSE]
ValidationSet  <- ValidationSet[, AllNA==FALSE]
dim(TrainSet)
```

```{r}
dim(ValidationSet)
```

##### Remove identification only variables (columns 1 to 5)
```{r}
TrainSet <- TrainSet[, -(1:5)]
ValidationSet  <- ValidationSet[, -(1:5)]
dim(TrainSet)
dim(ValidationSet)
```

##### Create working datasets from processed data.
```{r}
TrainSetWorking <- TrainSet
ValidationSetWorking <- ValidationSet
dim(TrainSetWorking)
dim(ValidationSetWorking)
```

## III - Correlation Analysis

Take a look at the correlation among variables.

```{r}
corMatrix <- cor(TrainSetWorking[, -54])
corrplot(corMatrix, order = "FPC", method = "color", type = "lower", 
         tl.cex = 0.8, tl.col = rgb(0, 0, 0))
```

## IV - Prediction model (using random forest)

In this step a prediction model is created using the Random Forest method. For the final knitted document I have chosen to not evaluate this chunk of code. When running the code for reproducibility this will have to be evaluated to continue processing data according to the layed out strategy.

```{r, eval= FALSE}
fit <- train(classe ~ ., method = "rf", data = TrainSetWorking)
save(fit, file = "modelFit.RData")
```

## IV - Measure the Accuracy and Sample Error of the prediction model

##### IV-A Training Data evaluation

Here a prediction will be created using the training set. Then we'll measure itâ€™s accuracy.

```{r}
load(file = "modelFit.RData", verbose = TRUE)
predTrain <- predict(fit, TrainSetWorking)
```

```{r}
confusionMatrix(predTrain, TrainSetWorking$classe)
```

##### IV-B Validation Data evaluation
Now that a prediction has been made on the testing data and the accuracy has been determined to be sufficient for testing, a test will be run using the validation set to determine the ability to predict properly what activity is being done and gauge the success of the actual test data.

```{r}
predValidation <- predict(fit, ValidationSetWorking)
confusionMatrix(predValidation, ValidationSetWorking$classe)
```
Using the validation subset to test the prediction model the accuracy is high at 99%. The out of sample error rate is very low at 0.0019. 

The following lists important predictors from the model. Also included is a summary of the prediction model.

```{r}
varImp(fit)
fit$finalModel
```

With the OOB Estimated Error of .19% and the high accuracy of the model using the validation data it is safe to proceed with applying the model to the testing data

## V Apply the prediction model to the test data

The last thing to do is run the test data and see how good the prediction model performed. Evaluation for this chunk is turned off.

```{r, eval=FALSE}
predTesting <- predict(fit, newdata = testing)
predTesting
```

